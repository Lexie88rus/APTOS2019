{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function, division, absolute_import\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom collections import OrderedDict\n\n# import data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import image utils\nfrom PIL import Image\nimport cv2\n\n# import image processing\nimport scipy.ndimage as ndi\nimport scipy\n\n# import image utilities\nfrom skimage.morphology import binary_opening, disk, label, binary_closing\n\n# import image augmentation\nfrom albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, Flip, OneOf, Compose, PadIfNeeded, RandomContrast, RandomGamma, RandomBrightness, ElasticTransform\n)\n\n# Import PyTorch\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import Optimizer\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data.sampler import SubsetRandomSampler, Sampler\nfrom torch.autograd import Variable\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\n\nimport math\nimport os\n\nimport time\n\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setup the environment:\n* paths to folder with data\n* load csv with data\n* setup the transforms"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/aptos2019-blindness-detection/train_images/'\nTEST_PATH = '../input/aptos2019-blindness-detection/test_images/'\n\ntrain = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n\ntrain_transforms = Compose([HorizontalFlip(p=0.5),\n                            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=30, p=0.2),\n                            OneOf([RandomContrast(),\n                                   RandomGamma(),\n                                   RandomBrightness(),], p=0.3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seup the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image_from_gray(img, tol=7):\n    \"\"\"\n    Applies masks to the orignal image and \n    returns the a preprocessed image with \n    3 channels\n    \"\"\"\n    # If for some reason we only have two channels\n    if img.ndim == 2:\n        mask = img > tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    # If we have a normal RGB images\n    elif img.ndim == 3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img > tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\ndef preprocess_image(image, sigmaX=10, img_width = 299, img_height = 299):\n    \"\"\"\n    The whole preprocessing pipeline:\n    1. Read in image\n    2. Apply masks\n    3. Resize image to desired size\n    4. Add Gaussian noise to increase Robustness\n    \"\"\"\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    #image = cv2.resize(image, (img_width, img_height))\n    #image = cv2.addWeighted (image,4, cv2.GaussianBlur(image, (0,0) ,sigmaX), -4, 128)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AptosDataset(Dataset):\n    '''\n    The dataset for APTOS data.\n    '''\n    def __init__(self, dataset_path, df, transforms=None, size = (299, 299), mode = 'train', indices = None):\n        self.dataset_path = dataset_path\n        self.df = df\n        self.transforms = transforms\n        self.size = size\n        self.mode = mode\n        self.indices = list(range(len(df))) \\\n            if indices is None else indices\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        \n        # open image and label\n        idx = self.indices[idx]\n        id_code = self.df.loc[idx]['id_code']\n        image = Image.open(os.path.join(self.dataset_path, str(id_code) + '.png'))\n        \n        if self.mode == 'train' or self.mode == 'test':\n            label = self.df.loc[idx]['diagnosis']\n        else:\n            label = None\n            \n        image = preprocess_image(np.asarray(image), self.size[0], self.size[1])\n        \n        # augment image\n        if self.mode == 'train':\n            augmented = self.transforms(image=np.array(image))\n            image = Image.fromarray(augmented['image'], 'RGB')\n        else:\n            image = Image.fromarray(image, 'RGB')\n        \n        # normalize and convert to tensor\n        tf = transforms.Compose([transforms.Resize(self.size),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize(mean=[0.500, 0.500, 0.500], std=[0.500, 0.500, 0.500])])\n        image = tf(image)\n        \n        # in validation mode return image and id_code\n        if self.mode == 'validation':\n            return image, id_code\n\n        # return tensor with image and label\n        return image, label\n    \n    def get_label(self, idx):\n        if not self.mode == 'validation':\n            idx = self.indices[idx]\n            label = self.df.loc[idx]['diagnosis']\n        else:\n            return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImbalancedDatasetSampler(Sampler):\n    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n    Arguments:\n        indices (list, optional): a list of indices\n        num_samples (int, optional): number of samples to draw\n    \"\"\"\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n                \n        # if indices is not provided, \n        # all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n            \n        # if num_samples is not provided, \n        # draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices) \\\n            if num_samples is None else num_samples\n            \n        # distribution of classes in the dataset \n        label_to_count = {}\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n                \n        # weight for each sample\n        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def _get_label(self, dataset, idx):\n        return dataset.get_label(idx)\n                \n    def __iter__(self):\n        return (self.indices[i] for i in torch.multinomial(\n            self.weights, self.num_samples, replacement=True))\n\n    def __len__(self):\n        return self.num_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weighted kappa metric\n\nfrom sklearn.metrics import cohen_kappa_score\n\ndef kappa_metric(label, pred):\n    pred = np.argmax(pred, axis = 1)\n    return cohen_kappa_score(label, pred, weights = 'quadratic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XceptionNet from [pretrained-models.pytorch repo](https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/xception.py)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n@author: tstandley\nAdapted by cadene\nCreates an Xception Model as defined in:\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\nREMEMBER to set your image size to 3x299x299 for both test and validation\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn import init\n\n__all__ = ['xception']\n\npretrained_settings = {\n    'xception': {\n        'imagenet': {\n            'url': '../input/aptos-preprocessed-data/xception-43020ad28.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1000,\n            'scale': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n        }\n    }\n}\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n\n        rep=[]\n\n        filters=in_filters\n        if grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x+=skip\n        return x\n\n\nclass Xception(nn.Module):\n    \"\"\"\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    \"\"\"\n    def __init__(self, num_classes=1000):\n        \"\"\" Constructor\n        Args:\n            num_classes: number of classes\n        \"\"\"\n        super(Xception, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        #do relu here\n\n        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n\n        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n\n        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n        self.bn3 = nn.BatchNorm2d(1536)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        #do relu here\n        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n        self.bn4 = nn.BatchNorm2d(2048)\n\n        self.fc = nn.Linear(2048, num_classes)\n\n        # #------- init weights --------\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n        # #-----------------------------\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        return x\n\n    def logits(self, features):\n        x = nn.ReLU(inplace=True)(features)\n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef xception(num_classes=1000, pretrained='imagenet'):\n    model = Xception(num_classes=num_classes)\n    if pretrained:\n        settings = pretrained_settings['xception'][pretrained]\n\n        model = Xception(num_classes=num_classes)\n        model.load_state_dict(torch.load(settings['url']))\n\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n\n    # TODO: ugly\n    model.last_linear = model.fc\n    del model.fc\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize pretrained XceptionNet with imagenet weights\nxception_model = xception(num_classes=1000, pretrained='imagenet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wrap up pretrained XceptionNet with new fc layer\nclass APTOSXception(nn.Module):\n\n    def __init__(self, xception_model, dropout = 0.3):\n        super(APTOSXception, self).__init__()\n        self.xception_model = xception_model\n        \n        self.classifier = nn.Sequential(OrderedDict([\n            ('fc1', nn.Linear(1000, 256)),\n            ('relu1', nn.ReLU(inplace=True)),\n            ('dropout1', nn.Dropout(dropout)),\n            ('fc2', nn.Linear(256, 128)),\n            ('bn2', nn.BatchNorm1d(num_features=128)),\n            ('relu2', nn.ReLU(inplace=True)),\n            ('dropout2', nn.Dropout(dropout)),\n            ('fc3', nn.Linear(128, 64)),\n            ('bn3', nn.BatchNorm1d(num_features=64)),\n            ('relu3', nn.ReLU(inplace=True)),\n            ('logits', nn.Linear(64, 5))]))\n    \n    def forward(self, input):\n        x = self.xception_model(input)\n        x = nn.ReLU(inplace=True)(x)\n        x = self.classifier(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the model\ndropout = 0.3\nmodel = APTOSXception(xception_model, dropout = dropout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup training device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define hyperparameters\ntest_split = 0.2\nbatch_size = 32\nepochs = 40\nlearning_rate = 0.001\nnum_workers = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataset and data loaders\ndataset_size = len(train)\nindices = list(range(dataset_size))\nsplit = int(np.floor(test_split * dataset_size))\nnp.random.seed(42)\nnp.random.shuffle(indices)\ntrain_indices, test_indices = indices[split:], indices[:split]\n\ntrain_ds = AptosDataset(TRAIN_PATH, train, transforms=train_transforms, size = (299, 299), mode = 'train', indices = train_indices)\ntest_ds = AptosDataset(TRAIN_PATH, train, transforms=None, size = (299, 299), mode = 'test', indices = test_indices)\n\ntrain_sampler = ImbalancedDatasetSampler(train_ds)\ntest_sampler = ImbalancedDatasetSampler(test_ds)\n\ntrainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\ntestloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set loss function\ncriterion = nn.CrossEntropyLoss()\n\n# set optimizer, only train the classifier parameters, feature parameters are frozen\nfor param in model.xception_model.parameters():\n    param.requires_grad = False\noptimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n\ntrain_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy', 'Train top-3 accuracy','Test loss', 'Test accuracy', 'Test top-3 accuracy', 'Kappa train', 'Kappa test']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, device, trainloader, testloader, epochs, criterion, optimizer, train_stats):\n    #train the model\n    model.to(device)\n    \n    # learning rate cosine annealing\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(trainloader), eta_min=0.000001)\n\n    steps = 0\n    running_loss = 0\n    for epoch in range(epochs):\n\n        since = time.time()\n\n        train_accuracy = 0\n        top3_train_accuracy = 0 \n        kappa_train = 0\n        \n        for inputs, labels in tqdm_notebook(trainloader):\n            steps += 1\n            # Move input and label tensors to the default device\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            logps = model.forward(inputs)\n            loss = criterion(logps, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # calculate train top-1 accuracy\n            ps = torch.exp(logps)\n            top_p, top_class = ps.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n\n            # Calculate train top-3 accuracy\n            np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n            target_numpy = labels.cpu().numpy()\n            top3_train_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n            \n            # Calculate train weighted kappa metric\n            kappa_train += kappa_metric(labels.cpu().numpy(), ps.detach().cpu().numpy())\n\n        time_elapsed = time.time() - since\n\n        test_loss = 0\n        test_accuracy = 0\n        top3_test_accuracy = 0\n        kappa_test = 0\n        model.eval()\n        with torch.no_grad():\n            for inputs, labels in testloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                logps = model.forward(inputs)\n                batch_loss = criterion(logps, labels)\n\n                test_loss += batch_loss.item()\n\n                # Calculate test top-1 accuracy\n                ps = torch.exp(logps)\n                top_p, top_class = ps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n\n                # Calculate test top-3 accuracy\n                np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n                target_numpy = labels.cpu().numpy()\n                top3_test_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n                \n                # Calculate test weighted kappa metric\n                kappa_test += kappa_metric(labels.cpu().numpy(), ps.detach().cpu().numpy())\n\n        print(f\"Epoch {epoch+1}/{epochs}.. \"\n              f\"Time per epoch: {time_elapsed:.4f}.. \"\n              f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n              f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n              f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n              f\"Top-3 train accuracy: {top3_train_accuracy/len(trainloader):.4f}.. \"\n              f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n              f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \"\n              f\"Top-3 test accuracy: {top3_test_accuracy/len(testloader):.4f}\"\n              f\"Weighed kappa test: {kappa_test/len(testloader):.4f}\"\n              f\"Weighed kappa train: {kappa_train/len(trainloader):.4f}\")\n\n        train_stats = train_stats.append({'Epoch': epoch + 1, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader), 'Train accuracy': train_accuracy/len(trainloader), 'Train top-3 accuracy':top3_train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader), 'Test accuracy': test_accuracy/len(testloader), 'Test top-3 accuracy':top3_test_accuracy/len(testloader), 'Kappa train':kappa_train/len(trainloader), 'Kappa test':kappa_test/len(testloader)}, ignore_index=True)\n\n        running_loss = 0\n        model.train()\n        scheduler.step()\n        \n    return model, train_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model\nmodel, train_stats = train_model(model, device, trainloader, testloader, epochs, criterion, optimizer, train_stats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view training stats\ntrain_stats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"validate the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_ds = AptosDataset(TEST_PATH, test, transforms=None, size = (299, 299), mode = 'validation')\nvalidloader = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = {'id_code': [], 'diagnosis': []}\n\nmodel.eval()\ntorch.cuda.empty_cache()\n\nfor X, id_codes in tqdm_notebook(validloader):\n    X = Variable(X).cuda()\n    output = model(X)\n    \n    for i, id_code in enumerate(id_codes):\n        top_p, top_class = output[i].topk(1)\n        diagnosis = top_class.cpu().numpy()\n        \n        submission['id_code'].append(id_code)\n        submission['diagnosis'].append(diagnosis[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(submission, columns=['id_code', 'diagnosis'])\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnosis_dict = {\n    0:'No DR',\n    1:'Mild',\n    2:'Moderate',\n    3: 'Severe',\n    4: 'Proliferative DR'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def view_classify(img, ps, title):\n    \"\"\"\n    Function for viewing an image and it's predicted classes\n    with matplotlib.\n\n    INPUT:\n        img - (tensor) image file\n        ps - (tensor) predicted probabilities for each class\n        title - (str) string with true label\n    \"\"\"\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(10,20), ncols=2)\n    image = img.permute(1, 2, 0)\n    ax1.imshow(image.numpy())\n    ax1.axis('off')\n    ax2.barh(np.arange(5), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(5))\n    ax2.set_yticklabels(list(diagnosis_dict.values()));\n    ax2.set_title(title)\n    ax2.set_xlim(0, 1.1)\n    plt.tight_layout()\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch_idx, (inputs, labels) in enumerate(testloader):\n    inputs, labels = inputs.to(device), labels.to(device)\n    img = inputs[0]\n    label_true = labels[0]\n    ps = model(inputs)\n    view_classify(img.cpu(), torch.softmax(ps[0].cpu(), dim=0), diagnosis_dict[int(label_true.cpu().numpy())])\n    \n    break;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\n\ncheckpoint = {'state_dict': model.state_dict()}\ntorch.save(checkpoint, 'Xception.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save training stats\ntrain_stats.to_csv('Xception_train_stats.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}